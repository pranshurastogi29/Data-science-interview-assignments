{"cells":[{"cell_type":"code","execution_count":null,"id":"4e247fc0","metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","execution":{"iopub.execute_input":"2021-12-26T13:43:16.454908Z","iopub.status.busy":"2021-12-26T13:43:16.453390Z","iopub.status.idle":"2021-12-26T13:43:23.332446Z","shell.execute_reply":"2021-12-26T13:43:23.331570Z","shell.execute_reply.started":"2021-12-26T13:39:04.651934Z"},"papermill":{"duration":6.907729,"end_time":"2021-12-26T13:43:23.332629","exception":false,"start_time":"2021-12-26T13:43:16.424900","status":"completed"},"tags":[],"id":"4e247fc0","outputId":"cc6c9d56-ba24-4992-a15d-df57bf60c05f"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n","  \n"]}],"source":["import os\n","import numpy as np \n","import pandas as pd \n","import re\n","import random\n","import string\n","from tqdm import tqdm\n","from tqdm.autonotebook import tqdm\n","from sklearn import model_selection\n","from sklearn import metrics\n","import tokenizers\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","from torch.optim import lr_scheduler\n","import transformers\n","from transformers import AdamW\n","from transformers import get_linear_schedule_with_warmup\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","from transformers import AutoTokenizer, AutoModel, AutoConfig"]},{"cell_type":"markdown","id":"409066f0","metadata":{"papermill":{"duration":0.023131,"end_time":"2021-12-26T13:43:23.379973","exception":false,"start_time":"2021-12-26T13:43:23.356842","status":"completed"},"tags":[],"id":"409066f0"},"source":["# Seed Up"]},{"cell_type":"code","source":["x = range(3, 6, -1)\n","for n in x:\n","  print(n)\n"],"metadata":{"id":"gz-i59WLOz6b","executionInfo":{"status":"ok","timestamp":1641218281932,"user_tz":-330,"elapsed":3,"user":{"displayName":"pranshu rastogi","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"12209237871877088716"}}},"id":"gz-i59WLOz6b","execution_count":1,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"87fad1fc","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:23.431792Z","iopub.status.busy":"2021-12-26T13:43:23.431076Z","iopub.status.idle":"2021-12-26T13:43:23.433033Z","shell.execute_reply":"2021-12-26T13:43:23.433392Z","shell.execute_reply.started":"2021-12-26T13:39:11.659983Z"},"papermill":{"duration":0.030308,"end_time":"2021-12-26T13:43:23.433543","exception":false,"start_time":"2021-12-26T13:43:23.403235","status":"completed"},"tags":[],"id":"87fad1fc"},"outputs":[],"source":["def seed_everything(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"]},{"cell_type":"code","execution_count":null,"id":"6f1de4df","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:23.484251Z","iopub.status.busy":"2021-12-26T13:43:23.483599Z","iopub.status.idle":"2021-12-26T13:43:23.488231Z","shell.execute_reply":"2021-12-26T13:43:23.487793Z","shell.execute_reply.started":"2021-12-26T13:39:11.669546Z"},"papermill":{"duration":0.031144,"end_time":"2021-12-26T13:43:23.488330","exception":false,"start_time":"2021-12-26T13:43:23.457186","status":"completed"},"tags":[],"id":"6f1de4df"},"outputs":[],"source":["seed=2020\n","seed_everything(seed)"]},{"cell_type":"markdown","id":"b845694d","metadata":{"papermill":{"duration":0.023662,"end_time":"2021-12-26T13:43:23.536461","exception":false,"start_time":"2021-12-26T13:43:23.512799","status":"completed"},"tags":[],"id":"b845694d"},"source":["# Config"]},{"cell_type":"code","execution_count":null,"id":"1f630a14","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:23.589787Z","iopub.status.busy":"2021-12-26T13:43:23.589008Z","iopub.status.idle":"2021-12-26T13:43:31.636276Z","shell.execute_reply":"2021-12-26T13:43:31.635724Z","shell.execute_reply.started":"2021-12-26T13:39:11.687126Z"},"papermill":{"duration":8.074972,"end_time":"2021-12-26T13:43:31.636413","exception":false,"start_time":"2021-12-26T13:43:23.561441","status":"completed"},"tags":[],"id":"1f630a14","outputId":"faf22661-f9fe-43ed-dc3a-b9f100bac2ce","colab":{"referenced_widgets":["2e615d9863294df784ba2c860d507daf","f45093c251a5458b8ebfd97b00f313eb","a71b2cad2851436892dbfc8bedb6ea84","a37022c2ef574798bd30b1303ab38165"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2e615d9863294df784ba2c860d507daf","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f45093c251a5458b8ebfd97b00f313eb","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a71b2cad2851436892dbfc8bedb6ea84","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a37022c2ef574798bd30b1303ab38165","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["TRAIN_BATCH_SIZE = 16\n","VALID_BATCH_SIZE = 32\n","EPOCHS = 6\n","TOKENIZER = AutoTokenizer.from_pretrained(\"roberta-base\",lowercase=True)"]},{"cell_type":"markdown","id":"64290366","metadata":{"papermill":{"duration":0.024968,"end_time":"2021-12-26T13:43:31.687596","exception":false,"start_time":"2021-12-26T13:43:31.662628","status":"completed"},"tags":[],"id":"64290366"},"source":["# Utils"]},{"cell_type":"code","execution_count":null,"id":"c1a828fa","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:31.746205Z","iopub.status.busy":"2021-12-26T13:43:31.745384Z","iopub.status.idle":"2021-12-26T13:43:31.747865Z","shell.execute_reply":"2021-12-26T13:43:31.747416Z","shell.execute_reply.started":"2021-12-26T13:39:19.791923Z"},"papermill":{"duration":0.035279,"end_time":"2021-12-26T13:43:31.747973","exception":false,"start_time":"2021-12-26T13:43:31.712694","status":"completed"},"tags":[],"id":"c1a828fa"},"outputs":[],"source":["class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"id":"9e46c67f","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:31.806166Z","iopub.status.busy":"2021-12-26T13:43:31.805367Z","iopub.status.idle":"2021-12-26T13:43:31.810345Z","shell.execute_reply":"2021-12-26T13:43:31.810744Z","shell.execute_reply.started":"2021-12-26T13:39:19.800895Z"},"papermill":{"duration":0.037724,"end_time":"2021-12-26T13:43:31.810874","exception":false,"start_time":"2021-12-26T13:43:31.773150","status":"completed"},"tags":[],"id":"9e46c67f"},"outputs":[],"source":["class EarlyStopping:\n","    def __init__(self, patience=7, mode=\"max\", delta=0.001):\n","        self.patience = patience\n","        self.counter = 0\n","        self.mode = mode\n","        self.best_score = None\n","        self.early_stop = False\n","        self.delta = delta\n","        if self.mode == \"min\":\n","            self.val_score = np.Inf\n","        else:\n","            self.val_score = -np.Inf\n","\n","    def __call__(self, epoch_score, model, model_path):\n","\n","        if self.mode == \"min\":\n","            score = -1.0 * epoch_score\n","        else:\n","            score = np.copy(epoch_score)\n","\n","        if self.best_score is None:\n","            self.best_score = score\n","            self.save_checkpoint(epoch_score, model, model_path)\n","        elif score < self.best_score + self.delta:\n","            self.counter += 1\n","            print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n","            if self.counter >= self.patience:\n","                self.early_stop = True\n","        else:\n","            self.best_score = score\n","            self.save_checkpoint(epoch_score, model, model_path)\n","            self.counter = 0\n","\n","    def save_checkpoint(self, epoch_score, model, model_path):\n","        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n","            print('Validation score improved ({} --> {}). Saving model!'.format(self.val_score, epoch_score))\n","            torch.save(model.state_dict(), model_path)\n","        self.val_score = epoch_score"]},{"cell_type":"code","execution_count":null,"id":"43bf2e10","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:31.865750Z","iopub.status.busy":"2021-12-26T13:43:31.865020Z","iopub.status.idle":"2021-12-26T13:43:31.866989Z","shell.execute_reply":"2021-12-26T13:43:31.867358Z","shell.execute_reply.started":"2021-12-26T13:39:19.818368Z"},"papermill":{"duration":0.030821,"end_time":"2021-12-26T13:43:31.867492","exception":false,"start_time":"2021-12-26T13:43:31.836671","status":"completed"},"tags":[],"id":"43bf2e10"},"outputs":[],"source":["def onehot(size, target):\n","    vec = torch.zeros(size, dtype=torch.long)\n","    vec[target] = 1.\n","    return vec"]},{"cell_type":"code","execution_count":null,"id":"0e48c4f4","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:31.922630Z","iopub.status.busy":"2021-12-26T13:43:31.921857Z","iopub.status.idle":"2021-12-26T13:43:31.925105Z","shell.execute_reply":"2021-12-26T13:43:31.924706Z","shell.execute_reply.started":"2021-12-26T13:39:19.828838Z"},"papermill":{"duration":0.03287,"end_time":"2021-12-26T13:43:31.925206","exception":false,"start_time":"2021-12-26T13:43:31.892336","status":"completed"},"tags":[],"id":"0e48c4f4"},"outputs":[],"source":["def jaccard(str1, str2): \n","    a = set(str1.lower().split()) \n","    b = set(str2.lower().split())\n","    c = a.intersection(b)\n","    return float(len(c)) / (len(a) + len(b) - len(c))"]},{"cell_type":"markdown","id":"3b483248","metadata":{"papermill":{"duration":0.025044,"end_time":"2021-12-26T13:43:31.975142","exception":false,"start_time":"2021-12-26T13:43:31.950098","status":"completed"},"tags":[],"id":"3b483248"},"source":["# Model"]},{"cell_type":"markdown","id":"b3fd69bc","metadata":{"papermill":{"duration":0.02465,"end_time":"2021-12-26T13:43:32.025207","exception":false,"start_time":"2021-12-26T13:43:32.000557","status":"completed"},"tags":[],"id":"b3fd69bc"},"source":["# Loss"]},{"cell_type":"code","execution_count":null,"id":"37ad45fd","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.081428Z","iopub.status.busy":"2021-12-26T13:43:32.080624Z","iopub.status.idle":"2021-12-26T13:43:32.083055Z","shell.execute_reply":"2021-12-26T13:43:32.082618Z","shell.execute_reply.started":"2021-12-26T13:39:19.838545Z"},"papermill":{"duration":0.032714,"end_time":"2021-12-26T13:43:32.083156","exception":false,"start_time":"2021-12-26T13:43:32.050442","status":"completed"},"tags":[],"id":"37ad45fd"},"outputs":[],"source":["def loss_fn(start_logits, end_logits, sent_logits, start_positions, end_positions, sentiments):\n","    loss_fct = nn.CrossEntropyLoss()\n","    start_loss = loss_fct(start_logits, start_positions)\n","    end_loss = loss_fct(end_logits, end_positions)\n","    total_loss = (start_loss + end_loss)/2\n","    sent_loss = loss_fct(sent_logits, sentiments)\n","    return torch.mean(torch.stack([total_loss, sent_loss]))"]},{"cell_type":"markdown","id":"9796a738","metadata":{"papermill":{"duration":0.024691,"end_time":"2021-12-26T13:43:32.133111","exception":false,"start_time":"2021-12-26T13:43:32.108420","status":"completed"},"tags":[],"id":"9796a738"},"source":["# Encoding Data"]},{"cell_type":"code","execution_count":null,"id":"24dee875","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.193383Z","iopub.status.busy":"2021-12-26T13:43:32.191232Z","iopub.status.idle":"2021-12-26T13:43:32.195796Z","shell.execute_reply":"2021-12-26T13:43:32.195297Z","shell.execute_reply.started":"2021-12-26T13:39:19.852080Z"},"papermill":{"duration":0.037655,"end_time":"2021-12-26T13:43:32.195918","exception":false,"start_time":"2021-12-26T13:43:32.158263","status":"completed"},"tags":[],"id":"24dee875"},"outputs":[],"source":["def process_data(text, selected_text, sentiment, tokenizer, max_len):\n","    text = \" \" + \" \".join(str(text).split())\n","    selected_text = \" \" + \" \".join(str(selected_text).split())\n","\n","    len_st = len(selected_text) - 1\n","    idx0 = None\n","    idx1 = None\n","\n","    for ind in (i for i, e in enumerate(text) if e == selected_text[1]):\n","        if \" \" + text[ind: ind+len_st] == selected_text:\n","            idx0 = ind\n","            idx1 = ind + len_st - 1\n","            break\n","\n","    char_targets = [0] * len(text)\n","    if idx0 != None and idx1 != None:\n","        for ct in range(idx0, idx1 + 1):\n","            char_targets[ct] = 1\n","    \n","    tok_text = tokenizer.encode_plus(text, add_special_tokens=True, \n","            max_length=max_len, \n","            pad_to_max_length=True,\n","            return_offsets_mapping = True,\n","            return_token_type_ids = True)\n","    \n","    text_offsets = tok_text['offset_mapping']\n","    \n","    target_idx = []\n","    for j, (offset1, offset2) in enumerate(text_offsets):\n","        if sum(char_targets[offset1: offset2]) > 0:\n","            target_idx.append(j)\n","    \n","    targets_start = target_idx[0]\n","    targets_end = target_idx[-1]    \n","    input_ids = tok_text['input_ids']\n","    token_type_ids = tok_text['token_type_ids']\n","    mask = tok_text['attention_mask']\n","    text_offsets = text_offsets\n","\n","    return {\n","        'ids': input_ids,\n","        'mask': mask,\n","        'token_type_ids': token_type_ids,\n","        'targets_start': targets_start,\n","        'targets_end': targets_end,\n","        'orig_text': text,\n","        'orig_selected': selected_text,\n","        'sentiment': int(sentiment),\n","        'offsets': text_offsets\n","        }"]},{"cell_type":"code","execution_count":null,"id":"f5629eef","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.254580Z","iopub.status.busy":"2021-12-26T13:43:32.253787Z","iopub.status.idle":"2021-12-26T13:43:32.255861Z","shell.execute_reply":"2021-12-26T13:43:32.256310Z","shell.execute_reply.started":"2021-12-26T13:39:19.869761Z"},"papermill":{"duration":0.035665,"end_time":"2021-12-26T13:43:32.256431","exception":false,"start_time":"2021-12-26T13:43:32.220766","status":"completed"},"tags":[],"id":"f5629eef"},"outputs":[],"source":["class ABSADataset:\n","    def __init__(self, text, sentiment, selected_text):\n","        self.text = text\n","        self.sentiment = sentiment\n","        self.selected_text = selected_text\n","        self.tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\",lowercase=True)\n","        self.max_len = 256\n","    \n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, item):\n","        data = process_data(\n","            self.text[item], \n","            self.selected_text[item],\n","            self.sentiment[item],\n","            self.tokenizer,\n","            self.max_len\n","        )\n","        \n","        return {\n","            'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n","            'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n","            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n","            'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n","            'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n","            'orig_text': data[\"orig_text\"],\n","            'orig_selected': data[\"orig_selected\"],\n","            'sentiment': torch.tensor(data['sentiment'], dtype=torch.long),\n","            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n","        }"]},{"cell_type":"markdown","id":"b92ed50f","metadata":{"papermill":{"duration":0.025046,"end_time":"2021-12-26T13:43:32.306290","exception":false,"start_time":"2021-12-26T13:43:32.281244","status":"completed"},"tags":[],"id":"b92ed50f"},"source":["# Decoder"]},{"cell_type":"code","execution_count":null,"id":"c29190d4","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.364005Z","iopub.status.busy":"2021-12-26T13:43:32.363285Z","iopub.status.idle":"2021-12-26T13:43:32.365772Z","shell.execute_reply":"2021-12-26T13:43:32.365277Z","shell.execute_reply.started":"2021-12-26T13:39:19.891266Z"},"papermill":{"duration":0.033195,"end_time":"2021-12-26T13:43:32.365874","exception":false,"start_time":"2021-12-26T13:43:32.332679","status":"completed"},"tags":[],"id":"c29190d4"},"outputs":[],"source":["def calculate_jaccard_score(\n","    original_text, \n","    target_string, \n","    idx_start, \n","    idx_end, \n","    offsets,\n","    verbose=False):\n","    \n","    if idx_end < idx_start:\n","        idx_end = idx_start\n","    \n","    filtered_output  = \"\"\n","    for ix in range(idx_start, idx_end + 1):\n","        filtered_output += original_text[offsets[ix][0]: offsets[ix][1]]\n","        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n","            filtered_output += \" \"\n","\n","    if len(original_text.split()) < 2:\n","        filtered_output = original_text\n","\n","    #jac = 0\n","    jac = jaccard(target_string.strip(), filtered_output.strip())\n","    return jac"]},{"cell_type":"markdown","id":"bffe3edb","metadata":{"papermill":{"duration":0.025067,"end_time":"2021-12-26T13:43:32.415773","exception":false,"start_time":"2021-12-26T13:43:32.390706","status":"completed"},"tags":[],"id":"bffe3edb"},"source":["# Evaluation"]},{"cell_type":"code","execution_count":null,"id":"d3197a7b","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.480668Z","iopub.status.busy":"2021-12-26T13:43:32.479874Z","iopub.status.idle":"2021-12-26T13:43:32.482211Z","shell.execute_reply":"2021-12-26T13:43:32.481825Z","shell.execute_reply.started":"2021-12-26T13:39:19.906739Z"},"papermill":{"duration":0.041493,"end_time":"2021-12-26T13:43:32.482309","exception":false,"start_time":"2021-12-26T13:43:32.440816","status":"completed"},"tags":[],"id":"d3197a7b"},"outputs":[],"source":["def eval_fn(data_loader, model, device):\n","    \n","    model.eval()\n","    losses = AverageMeter()\n","    jaccards = AverageMeter()\n","    accuracy = AverageMeter()\n","    \n","    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Validating\")\n","    \n","    with torch.no_grad():\n","        for bi, d in enumerate(tk0):\n","            ids = d[\"ids\"]\n","            token_type_ids = d[\"token_type_ids\"]\n","            mask = d[\"mask\"]\n","            sentiment = d[\"sentiment\"]\n","            orig_selected = d[\"orig_selected\"]\n","            orig_text = d[\"orig_text\"]\n","            targets_start = d[\"targets_start\"]\n","            targets_end = d[\"targets_end\"]\n","            offsets = d[\"offsets\"].cpu().numpy()\n","\n","            ids = ids.to(device, dtype=torch.long)\n","            token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","            mask = mask.to(device, dtype=torch.long)\n","            \n","            targets_start = targets_start.to(device, dtype=torch.long)\n","            targets_end = targets_end.to(device, dtype=torch.long)\n","            sentiment = sentiment.to(device, dtype=torch.long)\n","\n","            outputs_start, outputs_end, outputs_sent = model(\n","                ids=ids,\n","                mask=mask,\n","                token_type_ids=token_type_ids\n","            )\n","            \n","            loss = loss_fn(outputs_start, outputs_end, outputs_sent, targets_start, targets_end, sentiment)\n","            outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n","            outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n","            outputs_sent = torch.softmax(outputs_sent, dim=1).cpu().detach().numpy().argmax(axis=1)\n","            \n","            jaccard_scores = []\n","            for px, text in enumerate(orig_text):\n","                selected_text = orig_selected[px]\n","                text_sentiment = sentiment[px]\n","                jaccard_score = calculate_jaccard_score(\n","                    original_text=text,\n","                    target_string=selected_text,\n","                    idx_start=np.argmax(outputs_start[px, :]),\n","                    idx_end=np.argmax(outputs_end[px, :]),\n","                    offsets=offsets[px]\n","                )\n","                jaccard_scores.append(jaccard_score)\n","            \n","            acc = metrics.accuracy_score(sentiment.cpu().detach().numpy(), outputs_sent)\n","\n","            jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","            losses.update(loss.item(), ids.size(0))\n","            accuracy.update(acc, ids.size(0))\n","        tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg,accuracy=accuracy.avg)\n","\n","            \n","    return losses.avg, jaccards.avg, accuracy.avg"]},{"cell_type":"markdown","id":"2195dd96","metadata":{"papermill":{"duration":0.024938,"end_time":"2021-12-26T13:43:32.532452","exception":false,"start_time":"2021-12-26T13:43:32.507514","status":"completed"},"tags":[],"id":"2195dd96"},"source":["# Training function"]},{"cell_type":"code","execution_count":null,"id":"84f3c435","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.597704Z","iopub.status.busy":"2021-12-26T13:43:32.589649Z","iopub.status.idle":"2021-12-26T13:43:32.599739Z","shell.execute_reply":"2021-12-26T13:43:32.600140Z","shell.execute_reply.started":"2021-12-26T13:39:19.932636Z"},"papermill":{"duration":0.0427,"end_time":"2021-12-26T13:43:32.600260","exception":false,"start_time":"2021-12-26T13:43:32.557560","status":"completed"},"tags":[],"id":"84f3c435"},"outputs":[],"source":["def train_fn(data_loader, model, optimizer, device, scheduler=None):\n","    model.train()\n","    \n","    losses = AverageMeter()\n","    jaccards = AverageMeter()\n","    accuracy = AverageMeter()\n","    \n","    tk0 = tqdm(data_loader, total=len(data_loader), desc=\"Training\")\n","    \n","    for bi, d in enumerate(tk0):\n","        ids = d[\"ids\"]\n","        token_type_ids = d[\"token_type_ids\"]\n","        mask = d[\"mask\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        sentiment = d[\"sentiment\"]\n","        orig_selected = d[\"orig_selected\"]\n","        orig_text = d[\"orig_text\"]\n","        targets_start = d[\"targets_start\"]\n","        targets_end = d[\"targets_end\"]\n","        offsets = d[\"offsets\"]\n","        \n","        ids = ids.to(device, dtype=torch.long)\n","        token_type_ids = token_type_ids.to(device, dtype=torch.long)\n","        mask = mask.to(device, dtype=torch.long)\n","        \n","        targets_start = targets_start.to(device, dtype=torch.long)\n","        targets_end = targets_end.to(device, dtype=torch.long)\n","        sentiment = sentiment.to(device, dtype=torch.long)\n","\n","        model.zero_grad()\n","        \n","        outputs_start, outputs_end, outputs_sent = model(\n","            ids=ids,\n","            mask=mask,\n","            token_type_ids=token_type_ids,\n","        )\n","        \n","        loss = loss_fn(outputs_start, outputs_end, outputs_sent, \\\n","                       targets_start, targets_end, sentiment)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        \n","        outputs_start = torch.softmax(outputs_start, dim=1).cpu().detach().numpy()\n","        outputs_end = torch.softmax(outputs_end, dim=1).cpu().detach().numpy()\n","        outputs_sent = torch.softmax(outputs_sent, dim=1).cpu().detach().numpy().argmax(axis=1)\n","        \n","        jaccard_scores = []        \n","        for px, text in enumerate(orig_text):\n","            selected_text = orig_selected[px]\n","            text_sentiment = sentiment[px]\n","            idx_start = np.argmax(outputs_start[px, :])\n","            idx_end = np.argmax(outputs_end[px, :])\n","            \n","            jaccard_score = calculate_jaccard_score(\n","                original_text=text,\n","                target_string=selected_text,\n","                idx_start=idx_start,\n","                idx_end=idx_end,\n","                offsets=offsets[px])\n","            jaccard_scores.append(jaccard_score)\n","        \n","            \n","        acc = metrics.accuracy_score(sentiment.cpu().detach().numpy(), outputs_sent)\n","\n","        jaccards.update(np.mean(jaccard_scores), ids.size(0))\n","        losses.update(loss.item(), ids.size(0))\n","        accuracy.update(acc, ids.size(0))\n","    tk0.set_postfix(loss=losses.avg, jaccard=jaccards.avg,accuracy = accuracy.avg)"]},{"cell_type":"markdown","id":"c1dcf2eb","metadata":{"papermill":{"duration":0.024753,"end_time":"2021-12-26T13:43:32.650906","exception":false,"start_time":"2021-12-26T13:43:32.626153","status":"completed"},"tags":[],"id":"c1dcf2eb"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"id":"3b32f5df","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.708131Z","iopub.status.busy":"2021-12-26T13:43:32.707623Z","iopub.status.idle":"2021-12-26T13:43:32.768092Z","shell.execute_reply":"2021-12-26T13:43:32.767367Z","shell.execute_reply.started":"2021-12-26T13:39:19.956403Z"},"papermill":{"duration":0.091835,"end_time":"2021-12-26T13:43:32.768201","exception":false,"start_time":"2021-12-26T13:43:32.676366","status":"completed"},"tags":[],"id":"3b32f5df","outputId":"ee6ece16-f686-42eb-88be-dbe3362356b7"},"outputs":[{"name":"stdout","output_type":"stream","text":["3200 800\n","3200 800\n","3200 800\n","3200 800\n","3200 800\n"]}],"source":["from sklearn import model_selection\n","import re\n","\n","dfx = pd.read_csv(\"../input/enterpret/train.csv\")\n","dfx.columns = ['text','selected_text','sentiment']\n","dfx['selected_text'] = dfx['selected_text'].str.lower()\n","dfx['text'] = dfx['text'].str.lower()\n","\n","dfx = dfx.dropna().reset_index(drop=True)\n","dfx[\"kfold\"] = -1\n","\n","dfx = dfx.sample(frac=1,random_state=seed).reset_index(drop=True)\n","\n","kf = model_selection.StratifiedKFold(n_splits=5,random_state=seed)\n","\n","for fold, (trn_, val_) in enumerate(kf.split(X=dfx, y=dfx.sentiment.values)):\n","    print(len(trn_), len(val_))\n","    dfx.loc[val_, 'kfold'] = fold"]},{"cell_type":"code","execution_count":null,"id":"5dd4e040","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.825143Z","iopub.status.busy":"2021-12-26T13:43:32.824436Z","iopub.status.idle":"2021-12-26T13:43:32.826992Z","shell.execute_reply":"2021-12-26T13:43:32.826561Z","shell.execute_reply.started":"2021-12-26T13:39:20.043251Z"},"papermill":{"duration":0.033511,"end_time":"2021-12-26T13:43:32.827094","exception":false,"start_time":"2021-12-26T13:43:32.793583","status":"completed"},"tags":[],"id":"5dd4e040"},"outputs":[],"source":["def predict_aspect(\n","    original_text, \n","    target_string, \n","    idx_start, \n","    idx_end, \n","    offsets,\n","    verbose=False):\n","    \n","    if idx_end < idx_start:\n","        idx_end = idx_start\n","    \n","    filtered_output  = \"\"\n","    for ix in range(idx_start, idx_end + 1):\n","        filtered_output += original_text[offsets[ix][0]: offsets[ix][1]]\n","        if (ix+1) < len(offsets) and offsets[ix][1] < offsets[ix+1][0]:\n","            filtered_output += \" \"\n","\n","    if len(original_text.split()) < 2:\n","        filtered_output = original_text\n","\n","    return filtered_output, target_string"]},{"cell_type":"code","execution_count":null,"id":"393afc67","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.887491Z","iopub.status.busy":"2021-12-26T13:43:32.886827Z","iopub.status.idle":"2021-12-26T13:43:32.903450Z","shell.execute_reply":"2021-12-26T13:43:32.903895Z","shell.execute_reply.started":"2021-12-26T13:39:20.052279Z"},"papermill":{"duration":0.05163,"end_time":"2021-12-26T13:43:32.904010","exception":false,"start_time":"2021-12-26T13:43:32.852380","status":"completed"},"tags":[],"id":"393afc67"},"outputs":[],"source":["class ABSAModel(transformers.BertPreTrainedModel):\n","    def __init__(self, conf):\n","        super(ABSAModel, self).__init__(conf)\n","        self.backbone = AutoModel.from_pretrained('roberta-base', config=conf)\n","        \n","        self.d0 = nn.Dropout(0.1)\n","        self.d1 = nn.Dropout(0.1)\n","        \n","        self.l0 = nn.Linear(768 * 2, 2)\n","        self.l1 = nn.Linear(768 * 2, 3)\n","        \n","        torch.nn.init.normal_(self.l0.weight, std=0.02)\n","        torch.nn.init.normal_(self.l1.weight, std=0.02)\n","    \n","    def forward(self, ids, mask, token_type_ids):\n","        outputs = self.backbone(\n","            ids,\n","            attention_mask=mask,\n","            token_type_ids=token_type_ids)\n","        \n","        out = torch.stack(outputs[2])\n","        out = torch.cat((out[-1], out[-2]), dim=-1)\n","        \n","        # Head 1\n","        x = self.d0(out)\n","        logits = self.l0(x)\n","        start_logits, end_logits = logits.split(1, dim=-1)\n","        start_logits = start_logits.squeeze(-1)\n","        end_logits = end_logits.squeeze(-1)\n","        \n","        # Head 2\n","        y = self.d1(out)\n","        y = self.l1(y)\n","        y = y[:,0,:]\n","        sentiment = y.squeeze(-2)\n","\n","        return start_logits, end_logits, sentiment"]},{"cell_type":"code","execution_count":null,"id":"910d5423","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:32.968612Z","iopub.status.busy":"2021-12-26T13:43:32.967841Z","iopub.status.idle":"2021-12-26T13:43:32.969945Z","shell.execute_reply":"2021-12-26T13:43:32.970374Z","shell.execute_reply.started":"2021-12-26T13:39:20.093002Z"},"papermill":{"duration":0.04098,"end_time":"2021-12-26T13:43:32.970501","exception":false,"start_time":"2021-12-26T13:43:32.929521","status":"completed"},"tags":[],"id":"910d5423"},"outputs":[],"source":["def run(fold):\n","    df_train = dfx[dfx.kfold != fold].reset_index(drop=True).sample(frac= 1, random_state=2021)\n","    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True).sample(frac= 1, random_state=2021)\n","    \n","    train_dataset = ABSADataset(\n","        text=df_train.text.values,\n","        sentiment=df_train.sentiment.values,\n","        selected_text=df_train.selected_text.values\n","    )\n","\n","    train_data_loader = torch.utils.data.DataLoader(\n","        train_dataset,\n","        batch_size=TRAIN_BATCH_SIZE,\n","        shuffle=True,\n","        num_workers=4\n","    )\n","\n","    valid_dataset = ABSADataset(\n","        text=df_valid.text.values,\n","        sentiment=df_valid.sentiment.values,\n","        selected_text=df_valid.selected_text.values\n","    )\n","\n","    valid_data_loader = torch.utils.data.DataLoader(\n","        valid_dataset,\n","        shuffle = True,\n","        batch_size=VALID_BATCH_SIZE,\n","        num_workers=2\n","    )\n","\n","    device = torch.device(\"cuda\")\n","    model_config = AutoConfig.from_pretrained('roberta-base')\n","    model_config.output_hidden_states = True\n","    model = ABSAModel(conf=model_config)\n","    model.to(device)\n","\n","    num_train_steps = int(len(df_train) / TRAIN_BATCH_SIZE * EPOCHS)\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n","    optimizer_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n","    ]\n","    optimizer = AdamW(optimizer_parameters, lr=3e-5)\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, \n","        num_warmup_steps=0, \n","        num_training_steps=num_train_steps\n","    )\n","\n","    es = EarlyStopping(patience=4, mode=\"max\")\n","    print(f\"Training is Starting for fold={fold}\")\n","    \n","    for epoch in range(EPOCHS):\n","        train_fn(train_data_loader, model, optimizer, device, scheduler=scheduler)\n","        loss,jaccard,acc = eval_fn(valid_data_loader, model, device)\n","        print(f\"Jaccard Score = {jaccard}, accuracy = {acc}\")\n","        es(jaccard, model, model_path=f\"model_{fold}.bin\")\n","        if es.early_stop:\n","            print(\"Early stopping\")\n","            break"]},{"cell_type":"code","execution_count":null,"id":"1caa827c","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:43:33.779939Z","iopub.status.busy":"2021-12-26T13:43:33.775190Z","iopub.status.idle":"2021-12-26T13:53:06.274854Z","shell.execute_reply":"2021-12-26T13:53:06.276204Z"},"papermill":{"duration":572.530759,"end_time":"2021-12-26T13:53:06.276491","exception":false,"start_time":"2021-12-26T13:43:33.745732","status":"completed"},"tags":[],"id":"1caa827c","outputId":"c00eada3-f35c-4ce2-bf23-5d6bd30dccd9","colab":{"referenced_widgets":["396e21d4c5ff49aeb144a1e6e00a0319","0b79cde8ea0447fd9c05485d440bca9e","df89a8f5cc8343d3ab82208d7330f648","7e843b798365497ba85ed4ba4ed152ba","5b71c8a8030849b497f9a9406454776a","880d17ecd6a34b91b2b0819687d469da","533576b9a43b43928e736ad34a1f1255","5bf3548ecaae4be0bbb2294ea104ed3a","6af345586b75478296be511266b1ca89","043e9222b1a245e4a0fbfb5d307cae54","f1f47a53bcd346529931e583aaa287c7","6f9843f70f6d49d1a3554b93d5ff6ce4","4efeace2e8fa44c586b9af1e735e55d7"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"396e21d4c5ff49aeb144a1e6e00a0319","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/478M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Training is Starting for fold=0\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b79cde8ea0447fd9c05485d440bca9e","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df89a8f5cc8343d3ab82208d7330f648","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.423266060906686, accuracy = 0.66375\n","Validation score improved (-inf --> 0.423266060906686). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7e843b798365497ba85ed4ba4ed152ba","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5b71c8a8030849b497f9a9406454776a","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4304272561677454, accuracy = 0.71625\n","Validation score improved (0.423266060906686 --> 0.4304272561677454). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"880d17ecd6a34b91b2b0819687d469da","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"533576b9a43b43928e736ad34a1f1255","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.44848979911372155, accuracy = 0.735\n","Validation score improved (0.4304272561677454 --> 0.44848979911372155). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5bf3548ecaae4be0bbb2294ea104ed3a","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6af345586b75478296be511266b1ca89","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.43864030985354513, accuracy = 0.73125\n","EarlyStopping counter: 1 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"043e9222b1a245e4a0fbfb5d307cae54","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f1f47a53bcd346529931e583aaa287c7","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4423061822436823, accuracy = 0.725\n","EarlyStopping counter: 2 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f9843f70f6d49d1a3554b93d5ff6ce4","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4efeace2e8fa44c586b9af1e735e55d7","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4540409226190476, accuracy = 0.72625\n","Validation score improved (0.44848979911372155 --> 0.4540409226190476). Saving model!\n"]}],"source":["run(fold = 0)"]},{"cell_type":"code","execution_count":null,"id":"871e4221","metadata":{"execution":{"iopub.execute_input":"2021-12-26T13:53:06.375571Z","iopub.status.busy":"2021-12-26T13:53:06.375038Z","iopub.status.idle":"2021-12-26T14:02:20.745114Z","shell.execute_reply":"2021-12-26T14:02:20.746384Z"},"papermill":{"duration":554.419994,"end_time":"2021-12-26T14:02:20.746679","exception":false,"start_time":"2021-12-26T13:53:06.326685","status":"completed"},"tags":[],"id":"871e4221","outputId":"0ef188e8-8c8d-4ce5-a9f8-12ce7e8577f7","colab":{"referenced_widgets":["72e1d117b2e940f1afa03e637781e9b8","d402011b206143b0ad6e1b0c71dd850b","52d717fb2c3c404aa17b19913a06b2c3","6515d45bb855498d836b5bb5821eb26e","ad0ab655a3d74964a58f5f49926634e9","8fb3bff564524f18bc0afbfef6ee1dba","e04bdca00ca64992b800e74b545fa815","21e1aa7af87744ba90be7b9857598601","f5f001a22c6e4534b72a01081080bf78","dbc97ccba76645c7a977ccb233d513b5","eee9c3bec45d4d46b2ca6b993b020575","7b468aa9fb8f4e849b046ebc83f84075"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Training is Starting for fold=1\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"72e1d117b2e940f1afa03e637781e9b8","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d402011b206143b0ad6e1b0c71dd850b","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4261415234071484, accuracy = 0.62625\n","Validation score improved (-inf --> 0.4261415234071484). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"52d717fb2c3c404aa17b19913a06b2c3","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6515d45bb855498d836b5bb5821eb26e","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4599279678654679, accuracy = 0.7\n","Validation score improved (0.4261415234071484 --> 0.4599279678654679). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ad0ab655a3d74964a58f5f49926634e9","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8fb3bff564524f18bc0afbfef6ee1dba","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4663279212244539, accuracy = 0.71\n","Validation score improved (0.4599279678654679 --> 0.4663279212244539). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e04bdca00ca64992b800e74b545fa815","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"21e1aa7af87744ba90be7b9857598601","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.46831588067105306, accuracy = 0.71\n","Validation score improved (0.4663279212244539 --> 0.46831588067105306). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f5f001a22c6e4534b72a01081080bf78","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dbc97ccba76645c7a977ccb233d513b5","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4622003503680228, accuracy = 0.70125\n","EarlyStopping counter: 1 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eee9c3bec45d4d46b2ca6b993b020575","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7b468aa9fb8f4e849b046ebc83f84075","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4709164713997688, accuracy = 0.7025\n","Validation score improved (0.46831588067105306 --> 0.4709164713997688). Saving model!\n"]}],"source":["run(fold = 1)"]},{"cell_type":"code","execution_count":null,"id":"220b2fe2","metadata":{"execution":{"iopub.execute_input":"2021-12-26T14:02:20.881056Z","iopub.status.busy":"2021-12-26T14:02:20.877668Z","iopub.status.idle":"2021-12-26T14:11:34.806846Z","shell.execute_reply":"2021-12-26T14:11:34.806302Z"},"papermill":{"duration":553.994009,"end_time":"2021-12-26T14:11:34.806976","exception":false,"start_time":"2021-12-26T14:02:20.812967","status":"completed"},"tags":[],"id":"220b2fe2","outputId":"f1729ae7-1f7a-4e60-b9f2-3a7b1b16c988","colab":{"referenced_widgets":["71e0479a28904b79922000ad35a8d9c6","e0d3436ad65f44668016c5e818759c08","493aec53604e49fc86239bb836fe019e","8a2933a47bc24ee5915b3942dae6919c","fc8976e290e14457a244ff39afcb7d43","b0db54ff67ad4c9da58b7c0f2cf02261","ec74b2e891514df2a745c93a5b665f36","518745ac016643f2afd0564073a90b2a","7c2904907cb8465f98aa83ef86804511","92077c85be2c438881f4af052da3cd1e","2c6e4141b57f4a68a1cdca2a53296c02","562ef5846384424f90199ea7f21b986b"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Training is Starting for fold=2\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71e0479a28904b79922000ad35a8d9c6","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e0d3436ad65f44668016c5e818759c08","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.43982272330782385, accuracy = 0.69\n","Validation score improved (-inf --> 0.43982272330782385). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"493aec53604e49fc86239bb836fe019e","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8a2933a47bc24ee5915b3942dae6919c","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4586618056650951, accuracy = 0.69625\n","Validation score improved (0.43982272330782385 --> 0.4586618056650951). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fc8976e290e14457a244ff39afcb7d43","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b0db54ff67ad4c9da58b7c0f2cf02261","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4525564851814853, accuracy = 0.7275\n","EarlyStopping counter: 1 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec74b2e891514df2a745c93a5b665f36","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"518745ac016643f2afd0564073a90b2a","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4656270292207793, accuracy = 0.7375\n","Validation score improved (0.4586618056650951 --> 0.4656270292207793). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7c2904907cb8465f98aa83ef86804511","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"92077c85be2c438881f4af052da3cd1e","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4664359806859808, accuracy = 0.72625\n","EarlyStopping counter: 1 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2c6e4141b57f4a68a1cdca2a53296c02","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"562ef5846384424f90199ea7f21b986b","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4674072108447108, accuracy = 0.7375\n","Validation score improved (0.4656270292207793 --> 0.4674072108447108). Saving model!\n"]}],"source":["run(fold = 2)"]},{"cell_type":"code","execution_count":null,"id":"33329a35","metadata":{"execution":{"iopub.execute_input":"2021-12-26T14:11:34.971118Z","iopub.status.busy":"2021-12-26T14:11:34.970296Z","iopub.status.idle":"2021-12-26T14:20:48.205821Z","shell.execute_reply":"2021-12-26T14:20:48.206205Z"},"papermill":{"duration":553.322197,"end_time":"2021-12-26T14:20:48.206375","exception":false,"start_time":"2021-12-26T14:11:34.884178","status":"completed"},"tags":[],"id":"33329a35","outputId":"84db2b4b-90d3-424b-c9a8-26ae7bfcb36e","colab":{"referenced_widgets":["90584f7feac74a408719c455c3931e2a","4d58923171e54a80835016f2bb7f7827","76c3b06162e34c81a12f54f8e6199dd7","a1e69ae9d24342b5b5eb066183215e58","f4115b5b881b41c5a12bf47005a72cd1","330214af0b2a4a3f915be8cfab44e610","99ac865b5ddd40ce9cb67ad2ca298484","88fbb76b10824a259f1ba9828561ccb8","d4653ab490dc47f488200794858d2c04","dcd2838381244ee595f3906d4626a20d","5690468c2912425fa2db16c7e5f3eca3","9db92ccb11bb49de9dbca1d6992a073b"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Training is Starting for fold=3\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"90584f7feac74a408719c455c3931e2a","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4d58923171e54a80835016f2bb7f7827","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.42341241570929067, accuracy = 0.65875\n","Validation score improved (-inf --> 0.42341241570929067). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"76c3b06162e34c81a12f54f8e6199dd7","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a1e69ae9d24342b5b5eb066183215e58","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.45249746572871563, accuracy = 0.7175\n","Validation score improved (0.42341241570929067 --> 0.45249746572871563). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f4115b5b881b41c5a12bf47005a72cd1","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"330214af0b2a4a3f915be8cfab44e610","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4677316468253968, accuracy = 0.7225\n","Validation score improved (0.45249746572871563 --> 0.4677316468253968). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"99ac865b5ddd40ce9cb67ad2ca298484","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88fbb76b10824a259f1ba9828561ccb8","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4509185623404373, accuracy = 0.73\n","EarlyStopping counter: 1 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d4653ab490dc47f488200794858d2c04","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"dcd2838381244ee595f3906d4626a20d","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4600928422819871, accuracy = 0.7175\n","EarlyStopping counter: 2 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5690468c2912425fa2db16c7e5f3eca3","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9db92ccb11bb49de9dbca1d6992a073b","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.4520797561813186, accuracy = 0.71875\n","EarlyStopping counter: 3 out of 4\n"]}],"source":["run(fold = 3)"]},{"cell_type":"code","execution_count":null,"id":"85f0a7fe","metadata":{"execution":{"iopub.execute_input":"2021-12-26T14:20:48.393740Z","iopub.status.busy":"2021-12-26T14:20:48.392994Z","iopub.status.idle":"2021-12-26T14:30:00.516367Z","shell.execute_reply":"2021-12-26T14:30:00.515548Z"},"papermill":{"duration":552.219584,"end_time":"2021-12-26T14:30:00.516521","exception":false,"start_time":"2021-12-26T14:20:48.296937","status":"completed"},"tags":[],"id":"85f0a7fe","outputId":"8217bb8f-001a-495d-b47b-37d5162fc375","colab":{"referenced_widgets":["9982125cbbfb4f159b65209736b0d285","8de9dbca297644ecbc0311dad4ef148c","9dc8eeaa6d8d4eacb54b68647727040c","e478c4b02c3f41a09aaed22ebc1439a8","792f25408af24ae0bd2bf4e64f4cb72d","8942b257d9144e77aed3051aa561d874","a270f108786c43f18465df8eedc5e78c","14370717983f4f6b953e407db8c11ebc","1c5b2e6c4e674426be3d38a871f366ff","1e7b87b161e74ae19d0488ec6f1843df","205eabc8bf304c1c8cd71c57f27ef0cb","ac2fccadfbe7451cb4b3e66633e1b48e"]}},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Training is Starting for fold=4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9982125cbbfb4f159b65209736b0d285","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8de9dbca297644ecbc0311dad4ef148c","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.44153725354506607, accuracy = 0.65375\n","Validation score improved (-inf --> 0.44153725354506607). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9dc8eeaa6d8d4eacb54b68647727040c","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e478c4b02c3f41a09aaed22ebc1439a8","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.48690062368187353, accuracy = 0.70625\n","Validation score improved (0.44153725354506607 --> 0.48690062368187353). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"792f25408af24ae0bd2bf4e64f4cb72d","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8942b257d9144e77aed3051aa561d874","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.48142505292587534, accuracy = 0.71625\n","EarlyStopping counter: 1 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a270f108786c43f18465df8eedc5e78c","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"14370717983f4f6b953e407db8c11ebc","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.49136306401931407, accuracy = 0.72875\n","Validation score improved (0.48690062368187353 --> 0.49136306401931407). Saving model!\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1c5b2e6c4e674426be3d38a871f366ff","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1e7b87b161e74ae19d0488ec6f1843df","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.480996722027972, accuracy = 0.7175\n","EarlyStopping counter: 1 out of 4\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"205eabc8bf304c1c8cd71c57f27ef0cb","version_major":2,"version_minor":0},"text/plain":["Training:   0%|          | 0/200 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac2fccadfbe7451cb4b3e66633e1b48e","version_major":2,"version_minor":0},"text/plain":["Validating:   0%|          | 0/25 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"name":"stdout","output_type":"stream","text":["Jaccard Score = 0.47979217657342654, accuracy = 0.7075\n","EarlyStopping counter: 2 out of 4\n"]}],"source":["run(fold = 4)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"papermill":{"default_parameters":{},"duration":2816.310251,"end_time":"2021-12-26T14:30:04.256495","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2021-12-26T13:43:07.946244","version":"2.3.3"},"widgets":{"application/vnd.jupyter.widget-state+json":{}},"colab":{"name":"roberta-multi-task-learning-5-folds.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}